{"cells":[{"cell_type":"markdown","metadata":{"id":"kLTpDVQk0ozE"},"source":["# ECE 57000 Assignment 2 Exercises\n","\n","\n","\n","Name: Manish Kumar Krishne Gowda"]},{"cell_type":"markdown","metadata":{"id":"APWN2dfm7fx_"},"source":["# Important submission information\n","\n","1. Follow the instructions in the provided \"uploader.ipynb\" to convert your ipynb file into PDF format.\n","2. Please make sure to select the corresponding pages for each exercise when you submitting your PDF to Gradescope. Make sure to include both the **output** and the **code** when selecting pages. (You do not need to include the instruction for the exercises)\n","\n","\n","**We may assess a 20% penalty for those who do not correctly follow these steps.**"]},{"cell_type":"markdown","metadata":{"id":"cZ5oVoEqVnAD"},"source":["# 1. Task description & Background\n","## 1-1. Task description\n","\n","In this assignment, students will implement Stochastic Gradient Descent (SGD) for logistic regression and apply backpropagation for gradient descent/SGD on neural networks. You are only allowed to use basic functions or equivalent operations of NumPy package. The dataset from Assignment 1 will be reused.\n","\n","For the first part (logistic regression), students will define the model, loss function, compute gradients, and implement the SGD algorithm. In the second part, students will implement GD/SGD for a three-layer neural network, focusing on the forward pass and backpropagation.\n","\n","## 1-2. Background on dataset\n","In this assignment, we will explore the application of logistic regression to a binary classification problem in the field of medical diagnostics similar to the first assignment. The objective is to predict whether a breast tumor is benign or malignant based on features extracted from digitized images of fine needle aspirate (FNA) of breast mass.\n","\n","The dataset used is the Breast Cancer dataset from the UCI Machine Learning Repository, incorporated into scikit-learn as `load_breast_cancer`. This dataset includes measurements from 569 instances of breast tumors, with each instance described by 30 numeric attributes. These features include things like the texture, perimeter, smoothness, and symmetry of the tumor cells.\n","\n","You will split the data into training and test sets, with 80% of the data used for training and the remaining 20% for testing. This setup tests the modelâ€™s ability to generalize to new, unseen data. We set the `random_state` as 42 to ensure reproducibility. The logistic regression model, initialized with the 'liblinear' solver, will be trained on the training set.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yZCUDuPbVnAj"},"source":["# 2. Loading and preprocessing data from the previous assignment\n","\n","\n","You can load the Breast Cancer dataset by using [this function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) from the `sklearn.datasets` module (we have imported the function for you). Refer to the official documentation to understand more about this function.\n","\n","**Implement the Following:**\n","1.  `data`: Use the built-in function to load the dataset and store it in this variable.\n","2.  `X`: This should store the feature matrix from the dataset.\n","3.  `y`: This should store the target vector, which includes the labels indicating whether the tumor is benign or malignant.\n","\n","`X_train, X_test, y_train, y_test`: Split `X` and `y` into training and testing sets.\n","   - Set `test_size` to 0.2, allocating 20% of the data for testing.\n","   - Use `random_state=42` to ensure that your results are reproducible.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9XT8gbcVnAk","outputId":"1dfecca0-9b3e-491a-c993-970aa32efa94","executionInfo":{"status":"ok","timestamp":1727135840454,"user_tz":240,"elapsed":143,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The data has a shape of (569, 30), and the target has a shape of (569,)\n","The training set has (455, 30) datapoints and the test set has (114, 30) datapoints.\n","The max of training data is 1.00 and the min is 0.00.\n"]}],"source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","print(f'The data has a shape of {X.shape}, and the target has a shape of {y.shape}')\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","print(f'The training set has {X_train.shape[0]} datapoints and the test set has {X_test.shape[0]} datapoints.')\n","\n","scaler = MinMaxScaler().fit(X_train)\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)\n","print(f'The max of training data is {X_train.max():.2f} and the min is {X_train.min():.2f}.')\n"]},{"cell_type":"markdown","metadata":{"id":"GlKCbomPXEvo"},"source":["# 3. Initialize and train the logistic regression model with SGD (60/100 points)\n","\n","\n","You will initialize and train a logistic regression model.\n"]},{"cell_type":"markdown","metadata":{"id":"4-wFc_8AXL0L"},"source":["## 3-1. Defining sigmoid function and binary cross entropy function (10/100 points)\n","**Implement the Following:**\n","1. Sigmoid function: Implement the sigmoid function, which takes in a scalar or vector and returns the sigmoid of the input.\n","2. Binary Cross-Entropy Loss: Implement the binary cross-entropy loss function, which takes in the predictions and the true labels and returns the loss value. It is formulated as $\\ell(y,\\hat{y})=-\\frac{1}{N} \\sum_{n=1}^{N} \\left[ y_n( \\log \\hat{y}_n ) + (1-y_n) \\log (1-\\hat{y}_n) \\right]$.\n","\n","Please implement by using basic functions in numpy.\n","Ensure your code is placed between the comments `<Your code>` and `<end code>`. This structure is intended to keep your implementation organized and straightforward.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"2RaZ2U_WVnAl","executionInfo":{"status":"ok","timestamp":1727138057382,"user_tz":240,"elapsed":185,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}}},"outputs":[],"source":["import numpy as np\n","# initialize numpy random seed\n","np.random.seed(29)\n","\n","# Sigmoid function for logistic regression\n","def sigmoid(z):\n","    # <Your code>\n","    return 1 / (1 + np.exp(-z))\n","    # <end code>\n","\n","# Binary Cross-Entropy Loss\n","def binary_cross_entropy(y_true, y_pred):\n","    # Avoid log(0) by clipping predictions\n","    # <Your code>\n","    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n","    loss = - np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n","    return loss\n","    # <end code>"]},{"cell_type":"markdown","metadata":{"id":"S_nH8N9iWoRs"},"source":["## 3-2. Defining a class `LogisticRegression` based on SGD (50/100 points)\n","\n","**Implement the Following:**\n","1. Initialize Parameters: Implement `initialize_weights` function. Use zero initialization for `weights` and `bias`.\n","\n","2. predict function: Implement the predict function, which takes in the feature matrix `X` and returns the predicted value. Assuming $W \\in \\mathbb{R}^{D}$, $X \\in \\mathbb{R}^{N \\times D}$, $b \\in \\mathbb{R}^{1}$, the linear model is defined as $\\sigma(XW + b)$, where $\\sigma$ is sigmoid function.\n","\n","3. fit function: Implement the fit function, which trains the logistic regression model using SGD. The function should take in the feature matrix `X`, the true labels `y`, the learning rate `lr`, and the number of epochs `n_epochs`. In specific, first, at every epoch, you may shuffle indices of `n_samples` and reorganize the order of `X` and `y` to make sure that the order is randomized per epoch. Second, make a for loop for SGD. You may want to make a small batch data like `X_batch` and `y_batch`. Third, inside of the for loop for SGD, make a prediction by using the `predict` function you implemented. Fourth, compute the gradient with respect to `weights` and with respect to `bias`. Fifth, use the gradient to update `weights` and `bias`. In other words, implement the SGD algorithm $w^{(1)}=w^{(0)}-\\alpha \\nabla_w (\\text{BCE} (y, \\hat{y} ) )$ and $b^{(1)}=b^{(0)}-\\alpha \\nabla_b ( \\text{BCE} ( y,\\hat{y} ))$, where $\\alpha$ is a learning rate and $\\hat{y}$ is the prediction $\\sigma(XW + b)$. BCE indicates binary cross entropy loss.\n","\n","You are encouraged to experiment with different architectures and learning rates to see how they affect the performance of the model.   \n","Make sure you get accuracy greater than **0.85** on the test set."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"kfTGYGVIWinN","executionInfo":{"status":"ok","timestamp":1727138060612,"user_tz":240,"elapsed":193,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}}},"outputs":[],"source":["class LogisticRegression_SGD:\n","    def __init__(self, learning_rate=0.01, epochs=100, batch_size=32):\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.W = None\n","        self.b = None\n","\n","    # Initialize weights\n","    def initialize_weights(self, n_features):\n","        \"\"\"\n","        Initializes weights and bias to zero.\n","\n","        :param n_features: Number of input features\n","        \"\"\"\n","        # <Your code>\n","        self.W = np.zeros((n_features,))  # Zero initialization for weights\n","        self.b = 0.0                      # Zero initialization for bias\n","        # <end code>\n","\n","\n","    # Prediction function\n","    def predict(self, X):\n","        # <Your code>\n","        linear_model = np.dot(X, self.W) + self.b  # Linear model: XW + b\n","        predictions = sigmoid(linear_model)  # Apply sigmoid function\n","        return predictions\n","        # <end code>\n","\n","    # Training function using mini-batch SGD\n","    def fit(self, X, y):\n","        n_samples, n_features = X.shape\n","        self.initialize_weights(n_features)\n","\n","        for epoch in range(self.epochs):\n","            # Shuffle the data\n","            indices = np.arange(n_samples)\n","            np.random.shuffle(indices)\n","            X = X[indices]\n","            y = y[indices]\n","\n","            if epoch == 0:\n","              loss = binary_cross_entropy(y, self.predict(X))\n","              print(\"SGD loss\")\n","              print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")\n","\n","            for i in range(0, n_samples, self.batch_size):\n","                X_batch = X[i:i + self.batch_size]\n","                y_batch = y[i:i + self.batch_size]\n","\n","                # <Your code>\n","\n","                # Predictions\n","                y_pred = self.predict(X_batch)\n","\n","                # Compute gradients\n","                error = y_pred - y_batch  # Error term\n","                gradient_W = np.dot(X_batch.T, error) / self.batch_size  # Gradient with respect to weights\n","                gradient_b = np.mean(error)  # Gradient with respect to bias\n","\n","                # Update weights\n","                self.W -= self.learning_rate * gradient_W\n","                self.b -= self.learning_rate * gradient_b\n","\n","                # <end code>\n","\n","            # Calculate loss for monitoring\n","            loss = binary_cross_entropy(y, self.predict(X))\n","            if (epoch + 1) % 10 == 0:\n","              print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqqADhS7Vj2W","outputId":"656c9c2d-40cf-4be0-dbe2-ff24f50c18b6","executionInfo":{"status":"ok","timestamp":1727138063940,"user_tz":240,"elapsed":198,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["SGD loss\n","Epoch 1/100, Loss: 0.6931\n","Epoch 10/100, Loss: 0.3534\n","Epoch 20/100, Loss: 0.2714\n","Epoch 30/100, Loss: 0.2331\n","Epoch 40/100, Loss: 0.2094\n","Epoch 50/100, Loss: 0.1929\n","Epoch 60/100, Loss: 0.1811\n","Epoch 70/100, Loss: 0.1719\n","Epoch 80/100, Loss: 0.1638\n","Epoch 90/100, Loss: 0.1574\n","Epoch 100/100, Loss: 0.1518\n"]}],"source":["# You are encouraged to experiment with different architectures and learning rates to see how they affect the performance of the model.\n","# Training the model\n","model_SGD = LogisticRegression_SGD(learning_rate=0.1, epochs=100, batch_size=16)\n","model_SGD.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CTnF9h3yLUOW","outputId":"15252296-b34b-4c8b-9622-e2f346e24c88","executionInfo":{"status":"ok","timestamp":1727138068396,"user_tz":240,"elapsed":190,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n"," 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n"," 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n"," 1 1 0]\n","[1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n"," 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n"," 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n"," 1 1 0]\n","The accuracy is 0.9649\n"]}],"source":["# Code to check accuracy of your implementation\n","from sklearn.metrics import accuracy_score\n","\n","predictions = model_SGD.predict(X_test)\n","predictions = (predictions > 0.5).astype(int)\n","print(predictions)\n","print(y_test)\n","accuracy = accuracy_score(y_test, predictions)\n","\n","print(f'The accuracy is {accuracy:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"NXsERLmJVnAm"},"source":["# 4. 3-Layer Neural Network with SGD (40/100 points)\n","\n","\n","Now, we extend our 1-layer neural network to 3-layers neural network.\n","\n","**Implement the Following:**\n","\n","Ensure your code is placed between the comments `<Your code>` and `<end code>`. This structure is intended to keep your implementation organized and straightforward.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6Kk26XbDXL0N"},"source":["## 4-1. Defining activation functions and the derivative (10/100 points)\n","\n","**Implement the Following:**\n","1. relu function: Implement the ReLU activation function, which takes in a scalar or vector and returns the ReLU of the input.\n","2. relu_derivative function: Implement the derivative of the ReLU activation function, which takes in a scalar or vector and returns the derivative of the ReLU of the input.\n","3. sigmoid function: Implement the sigmoid activation function, which takes in a scalar or vector and returns the sigmoid of the input."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"gB2LyOiHYhWY","executionInfo":{"status":"ok","timestamp":1727138360957,"user_tz":240,"elapsed":263,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}}},"outputs":[],"source":["def relu(z):\n","    \"\"\"ReLU activation function.\"\"\"\n","    # <Your code>\n","    return np.maximum(0, z)\n","    # <end code>\n","\n","def relu_derivative(z):\n","    \"\"\"Derivative of ReLU activation function.\"\"\"\n","    # <Your code>\n","    return np.where(z > 0, 1, 0)\n","    # <end code>\n","\n","def sigmoid(z):\n","    \"\"\"Sigmoid activation function.\"\"\"\n","    # <Your code>\n","    return 1 / (1 + np.exp(-z))\n","    # <end code>"]},{"cell_type":"markdown","metadata":{"id":"ExSubVgGYrA5"},"source":["## 4-2. Defining 3-layer Neural Network (30/100 points)\n","\n","**Implement the Following:**\n","\n","1. Initialize Parameters: Implement `initialize_weights` function. Implement Kaiming initialization to initialize `weights`. Use zero initialization for `bias`.\n","\n","2. forward function: Compute the pre-activation for each layer by multiplying inputs or previous activations with weights and adding biases. Apply the ReLU activation function for hidden layers and the Sigmoid function for the output layer. Finally, return the activated output of the network. The formulation of forward function can be defined as:\n","\n","$$\\sigma(\\text{relu}(\\text{relu}(XW_1+b_1)W_2+b_2)W_3+b_3)$$\n","\n","3. backward function:\n","    1. **Compute Gradient of Loss**: Calculate the gradient of the loss with respect to the network's output.\n","    2. **Compute Gradients for Weights and Biases**: Use the gradients from the output to compute the gradients of weights and biases at each layer, applying the activation function's derivative where needed.\n","    3. **Propagate Gradients Backward**: Continue to backpropagate the gradients through the network, adjusting calculations as you move from one layer to the previous.\n","    4. **Update Parameters**: Update all weights and biases using the calculated gradients and learning rate.\n","\n","    This backpropagation adjusts the model parameters to minimize the loss.\n","\n","4. predict function: Implement the predict function, which takes in the feature matrix `X` and returns the predicted class (0 or 1).\n","\n","You are encouraged to experiment with different architectures and learning rates to see how they affect the performance of the model.\n","\n","Make sure you get accuracy greater than **0.75** on the test set."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"Uy8RcFLdVnAn","executionInfo":{"status":"ok","timestamp":1727142844268,"user_tz":240,"elapsed":145,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}}},"outputs":[],"source":["# Neural Network Model with an additional hidden layer\n","class NeuralNetwork:\n","    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, learning_rate=0.01, epochs=100, batch_size=32):\n","        \"\"\"\n","        Initialize the Neural Network with given parameters.\n","        :param input_size: Number of input features\n","        :param hidden_size1: Number of neurons in the first hidden layer\n","        :param hidden_size2: Number of neurons in the second hidden layer\n","        :param output_size: Number of output neurons (1 for binary classification)\n","        :param learning_rate: Learning rate for weight updates\n","        :param epochs: Number of training iterations\n","        :param batch_size: Size of mini-batches for SGD\n","        \"\"\"\n","        self.input_size = input_size\n","        self.hidden_size1 = hidden_size1\n","        self.hidden_size2 = hidden_size2\n","        self.output_size = output_size\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.W1 = None\n","        self.b1 = None\n","        self.W2 = None\n","        self.b2 = None\n","        self.W3 = None\n","        self.b3 = None\n","        self.A1 = None\n","        self.A2 = None\n","        self.Z1 = None\n","        self.Z2 = None\n","        self.initialize_weights()\n","\n","    def initialize_weights(self):\n","        \"\"\"Initialize weights and biases using Kaiming initialization.\"\"\"\n","        # <Your code>\n","        # Kaiming initialization for the first layer weights\n","        self.W1 = np.random.randn(self.input_size, self.hidden_size1) * np.sqrt(2. / self.input_size)\n","        self.b1 = np.zeros((1, self.hidden_size1))\n","\n","        # Kaiming initialization for the second layer weights\n","        self.W2 = np.random.randn(self.hidden_size1, self.hidden_size2) * np.sqrt(2. / self.hidden_size1)\n","        self.b2 = np.zeros((1, self.hidden_size2))\n","\n","        # Kaiming initialization for the third layer weights (output layer)\n","        self.W3 = np.random.randn(self.hidden_size2, self.output_size) * np.sqrt(2. / self.hidden_size2)\n","        self.b3 = np.zeros((1, self.output_size))\n","        # <end code>\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Forward pass through the network.\n","        :param X: Input data\n","        :return: Activated output of the network\n","        \"\"\"\n","        # <Your code>\n","        # First layer pre-activation: Z1 = XW1 + b1\n","        self.Z1 = np.dot(X, self.W1) + self.b1\n","        # First layer activation: A1 = relu(Z1)\n","        self.A1 = relu(self.Z1)\n","\n","        # Second layer pre-activation: Z2 = A1W2 + b2\n","        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n","        # Second layer activation: A2 = relu(Z2)\n","        self.A2 = relu(self.Z2)\n","\n","        # Output layer pre-activation: Z3 = A2W3 + b3\n","        Z3 = np.dot(self.A2, self.W3) + self.b3\n","        # Output layer activation: A3 = sigmoid(Z3)\n","        A3 = sigmoid(Z3)  # Sigmoid activation\n","        return A3\n","        # <end code>\n","\n","    def backward(self, X, y, output):\n","        \"\"\"\n","        Backpropagation to compute gradients and update weights.\n","        :param X: Input data\n","        :param y: True labels\n","        :param output: Predicted output from forward pass\n","        \"\"\"\n","        m = X.shape[0]\n","\n","        # Gradient of loss w.r.t. output (binary cross-entropy with sigmoid activation)\n","        dZ3 = output - y[:,None]  # Gradient wrt Z3 when using sigmoid activation at output\n","\n","        # <Your code>\n","        # Gradients for the third layer (output layer)\n","        dW3 = np.dot(self.A2.T, dZ3) / m  # Gradient wrt W3\n","        db3 = np.sum(dZ3, axis=0, keepdims=True) / m  # Gradient wrt b3\n","\n","        # Backpropagate through the second hidden layer\n","        dA2 = np.dot(dZ3, self.W3.T)  # Gradient wrt A2\n","        dZ2 = dA2 * relu_derivative(self.Z2)  # Applying ReLU derivative\n","        dW2 = np.dot(self.A1.T, dZ2) / m  # Gradient wrt W2\n","        db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # Gradient wrt b2\n","\n","        # Backpropagate through the first hidden layer\n","        dA1 = np.dot(dZ2, self.W2.T)  # Gradient wrt A1\n","        dZ1 = dA1 * relu_derivative(self.Z1)  # Applying ReLU derivative\n","        dW1 = np.dot(X.T, dZ1) / m  # Gradient wrt W1\n","        db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # Gradient wrt b1\n","\n","        # Update weights and biases using gradients\n","        self.W3 -= self.learning_rate * dW3\n","        self.b3 -= self.learning_rate * db3\n","        self.W2 -= self.learning_rate * dW2\n","        self.b2 -= self.learning_rate * db2\n","        self.W1 -= self.learning_rate * dW1\n","        self.b1 -= self.learning_rate * db1\n","\n","        # <end code>\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Train the neural network using mini-batch SGD.\n","        :param X: Training data\n","        :param y: True labels\n","        \"\"\"\n","        loss = binary_cross_entropy(y, self.forward(X))\n","        print(f\"Epoch 0/{self.epochs}, Loss: {loss:.4f}\")\n","\n","        for epoch in range(self.epochs):\n","            indices = np.arange(X.shape[0])\n","            np.random.shuffle(indices)\n","            X = X[indices]\n","            y = y[indices]\n","\n","            for i in range(0, X.shape[0], self.batch_size):\n","                X_batch = X[i:i + self.batch_size]\n","                y_batch = y[i:i + self.batch_size]\n","\n","                # Forward and backward pass\n","                output = self.forward(X_batch)\n","                self.backward(X_batch, y_batch, output)\n","\n","            # Calculate and print loss for monitoring\n","            if (epoch + 1) % 100 == 0:\n","              loss = binary_cross_entropy(y, self.forward(X))\n","              print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict using the trained neural network.\n","        :param X: Input data\n","        :return: Predicted labels\n","        \"\"\"\n","\n","        # <Your code>\n","        predictions = self.forward(X)\n","        return (predictions > 0.5).astype(int)  # Convert probabilities to binary predictions\n","        # <end code>\n"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnakwS_5LUOW","outputId":"6200d144-c741-426e-faa7-9c600be93040","executionInfo":{"status":"ok","timestamp":1727143502303,"user_tz":240,"elapsed":4086,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/1000, Loss: 0.6972\n","Epoch 100/1000, Loss: 0.6927\n","Epoch 200/1000, Loss: 0.6886\n","Epoch 300/1000, Loss: 0.6847\n","Epoch 400/1000, Loss: 0.6811\n","Epoch 500/1000, Loss: 0.6775\n","Epoch 600/1000, Loss: 0.6751\n","Epoch 700/1000, Loss: 0.6741\n","Epoch 800/1000, Loss: 0.6745\n","Epoch 900/1000, Loss: 0.6758\n","Epoch 1000/1000, Loss: 0.6782\n"]}],"source":["# You are encouraged to experiment with different architectures and learning rates to see how they affect the performance of the model.\n","nn_network = NeuralNetwork(input_size=X_train.shape[1], hidden_size1=8, hidden_size2=4, output_size=1, learning_rate=0.0001, epochs=1000, batch_size=16)\n","nn_network.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":90,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vjsW0iYKLUOW","outputId":"5e619590-62e0-4053-c6e6-a04aad947fdf","executionInfo":{"status":"ok","timestamp":1727143503485,"user_tz":240,"elapsed":159,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n"," 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1\n"," 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1\n"," 1 1 0]\n","[1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n"," 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n"," 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n"," 1 1 0]\n","The accuracy is 0.8421\n"]}],"source":["# Code to check accuracy of your implementation\n","predictions = nn_network.predict(X_test)\n","print(predictions.reshape(-1))\n","print(y_test)\n","accuracy = accuracy_score(y_test, predictions)\n","\n","print(f'The accuracy is {accuracy:.4f}')"]},{"cell_type":"code","source":["# You are encouraged to experiment with different architectures and learning rates to see how they affect the performance of the model.\n","nn_network = NeuralNetwork(input_size=X_train.shape[1], hidden_size1=8, hidden_size2=4, output_size=1, learning_rate=0.001, epochs=1000, batch_size=16)\n","nn_network.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hSLjUG3cxSXl","executionInfo":{"status":"ok","timestamp":1727143523708,"user_tz":240,"elapsed":6728,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}},"outputId":"4d6b2fb6-a0ff-46dc-f6f2-77fa1349b55c"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/1000, Loss: 0.7031\n","Epoch 100/1000, Loss: 0.6767\n","Epoch 200/1000, Loss: 0.6885\n","Epoch 300/1000, Loss: 0.7460\n","Epoch 400/1000, Loss: 0.8559\n","Epoch 500/1000, Loss: 0.9877\n","Epoch 600/1000, Loss: 1.1139\n","Epoch 700/1000, Loss: 1.2323\n","Epoch 800/1000, Loss: 1.3486\n","Epoch 900/1000, Loss: 1.4586\n","Epoch 1000/1000, Loss: 1.5648\n"]}]},{"cell_type":"code","source":["# Code to check accuracy of your implementation\n","predictions = nn_network.predict(X_test)\n","print(predictions.reshape(-1))\n","print(y_test)\n","accuracy = accuracy_score(y_test, predictions)\n","\n","print(f'The accuracy is {accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqvluhXuxaGi","executionInfo":{"status":"ok","timestamp":1727143527138,"user_tz":240,"elapsed":176,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}},"outputId":"a43612af-55db-4c4f-dc35-5927744b9650"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n"," 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n"," 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n"," 1 1 0]\n","[1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n"," 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n"," 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n"," 1 1 0]\n","The accuracy is 0.9649\n"]}]},{"cell_type":"code","source":["# You are encouraged to experiment with different architectures and learning rates to see how they affect the performance of the model.\n","nn_network = NeuralNetwork(input_size=X_train.shape[1], hidden_size1=4, hidden_size2=16, output_size=1, learning_rate=0.001, epochs=1000, batch_size=16)\n","nn_network.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65wE8r_bxuPF","executionInfo":{"status":"ok","timestamp":1727143550430,"user_tz":240,"elapsed":5117,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}},"outputId":"6686e750-baf8-43ce-e4c9-8b9dd1c2ed6c"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/1000, Loss: 0.6883\n","Epoch 100/1000, Loss: 0.6709\n","Epoch 200/1000, Loss: 0.6822\n","Epoch 300/1000, Loss: 0.7359\n","Epoch 400/1000, Loss: 0.8424\n","Epoch 500/1000, Loss: 0.9772\n","Epoch 600/1000, Loss: 1.1148\n","Epoch 700/1000, Loss: 1.2420\n","Epoch 800/1000, Loss: 1.3642\n","Epoch 900/1000, Loss: 1.4784\n","Epoch 1000/1000, Loss: 1.5860\n"]}]},{"cell_type":"code","source":["# Code to check accuracy of your implementation\n","predictions = nn_network.predict(X_test)\n","print(predictions.reshape(-1))\n","print(y_test)\n","accuracy = accuracy_score(y_test, predictions)\n","\n","print(f'The accuracy is {accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPg1FWCuxzGl","executionInfo":{"status":"ok","timestamp":1727143557415,"user_tz":240,"elapsed":147,"user":{"displayName":"Manish_Kumar Deep_Learning","userId":"10787978629583361420"}},"outputId":"f6f52b4f-cab0-4f65-b441-1f30b955a93c"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n"," 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n"," 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n"," 1 1 0]\n","[1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n"," 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n"," 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n"," 1 1 0]\n","The accuracy is 0.9737\n"]}]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}